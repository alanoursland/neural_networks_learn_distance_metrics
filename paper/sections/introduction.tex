\section{Introduction}

Neural networks have transformed the field of machine learning by learning complex, hierarchical representations of data. Since their inception, these models predominantly been interpreted through the lens of intensity-based representations, where larger activation values are assumed to correspond to stronger feature presence. This perspective traces back to the McCulloch-Pitts neuron \cite{mcculloch1943logical} and Rosenblatt's perceptron \cite{rosenblatt1958perceptron}, forming the basis of modern deep learning. However, the success of neural networks often outpaces our theoretical understanding of their internal mechanisms \cite{lipton2018mythos}. Central questions about how neural networks represent and process features remain unresolved, limiting our ability to interpret their behavior and design more effective architectures.

Recent theoretical work challenges the dominance of intensity-based interpretations, suggesting that distance-based representations may naturally emerge in neural networks \cite{oursland2024interpreting}. In a distance-based paradigm, smaller activations indicate proximity to learned prototypes or decision boundaries, fundamentally altering how we understand feature representations. This alternative perspective, grounded in statistical principles such as the Mahalanobis distance \cite{mahalanobis1936generalized}, provides a compelling framework for rethinking neural network behavior. However, the empirical exploration of this idea and its implications for network interpretability remain underdeveloped.

\textbf{Core Questions.} This paper addresses three key questions:
\begin{itemize}
    \item Do neural networks naturally prefer distance-based or intensity-based representations when learning features?
    \item How do architectural choices, such as activation functions and bias terms, shape representational biases?
    \item What are the geometric and statistical principles underlying these representational preferences, and how can they inform network design?
\end{itemize}

\textbf{Contributions.} To investigate these questions, we propose a theoretical framework that examines neural network behavior through the dual lenses of distance and intensity representations. We then validate this framework through empirical experiments designed to probe representational biases under different architectural constraints. Our contributions are as follows:
\begin{enumerate}
    \item \textit{Theoretical Framework:} We formalize the distinction between distance- and intensity-based representations, highlighting their implications for interpretability and network design.
    \item \textit{Empirical Analysis:} Through systematic experiments, we analyze network behavior under six architectural variants, uncovering the mechanisms behind catastrophic dead node creation under intensity-based constraints and providing geometric explanations for performance limitations under distance-based constraints.
    \item \textit{Validation via OffsetL2:} As a surprising and validating result, we introduce OffsetL2, a novel architecture that reinforces our theoretical framework by leveraging distance-based principles, achieving strong and stable performance while opening new directions for future research.
\end{enumerate}

These contributions provide a robust theoretical framework for understanding neural network representations, supported by empirical findings that validate the geometric principles through surprising experimental results.

The remainder of this paper is structured as follows. Section \ref{sec:related_work} reviews related work on neural network interpretability, distance metrics, and activation functions. Section \ref{sec:background} provides the theoretical foundation for distance- and intensity-based representations. Section \ref{sec:exp_design} outlines our experimental setup, including the six architectural variants designed to probe representational biases. Section \ref{sec:results} presents our empirical findings, with a focus on catastrophic dead node failures and the geometric analysis of learned representations. Finally, Section \ref{sec:discussion} synthesizes these results, introduces OffsetL2 as a validating conclusion, and discusses implications for future research.
