\section{Conclusion}

This work reimagines neural network feature representation through the lens of statistical distance, offering a compelling alternative to the dominant intensity-based paradigm. We challenge the assumption that larger activations inherently signify stronger feature presence, and instead explore the hypothesis that neural networks may be fundamentally biased towards learning distance-based representations, where smaller activations indicate proximity to learned prototypes.

First, our findings reveal that while networks can adopt either representation, ReLU-based architectures exhibit a strong bias towards distance-based learning. This bias, rooted in the geometric constraints of high-dimensional spaces, is further illuminated by theoretical analysis and validated by a novel architecture that explicitly models distances to learned prototypes. These insights open new avenues for designing more interpretable and effective neural network models.

Second, we provide theoretical and empirical evidence explaining why certain architectures succeed or fail under specific constraints. The significant performance drop of ReLU2 (to 47.20\% accuracy) reveals how intensity-based constraints can hinder learning distance-based representations, particularly when most inputs should lie near decision boundaries. Conversely, the performance gap between Abs2 (95.35\% accuracy) and Abs2-Neg (90.08\% accuracy) underscores the importance of combinatorial flexibility for learning distance-based representations in high-dimensional spaces. These examples illustrate how geometric interactions in the feature space, particularly those related to distance-based representations, rather than intrinsic properties of activation functions, drive network performance.

To further validate our theoretical framework, we developed OffsetL2, a novel architecture that directly addresses the limitations of Abs2-Neg by explicitly incorporating Mahalanobis distance calculations. By learning a single prototype, representing either the optimal $z_c$ or $z_{\neg c}$ for each class, OffsetL2 avoids the pitfalls of implicit prototype discovery through constrained hyperplanes. This approach not only achieved superior performance (97.61\% accuracy) and remarkable stability (Â± 0.07\% standard deviation) but also provides compelling evidence for the importance of directly modeling geometric relationships when learning distance-based representations.


The insights gained from this work have broad implications for neural network design and interpretability. By adopting a distance-based geometric perspective, we can begin to understand why certain architectures succeed or fail, and how seemingly minor changes, like the choice of activation function, can fundamentally alter the learned representation.  While this study focused on relatively simple two-layer networks and the MNIST dataset, it provides a compelling case for the importance of considering the geometric constraints imposed by high-dimensional spaces when designing and analyzing deeper, more complex models. Future work should explore how these principles apply to other datasets and architectures, particularly convolutional neural networks, where the interplay between local receptive fields and global structure may further illuminate the role of distance-based representations. 

The strong performance of OffsetL2, which directly models distances to learned prototypes, suggests that architectures explicitly incorporating statistical distance calculations hold significant promise. Further investigation into this new class of models, including exploring variations of the OffsetL2 architecture and applying it to diverse tasks, could lead to networks that are not only more powerful but also more transparent and aligned with the underlying statistical structure of the data. This research, therefore, opens new avenues for developing neural networks that are both more effective and easier to understand, bridging the gap between theoretical understanding and practical application.


In conclusion, this research offers a paradigm shift in how we understand neural network representations, advocating for a distance-based geometric perspective over the traditional intensity-based view. Our findings illuminate the crucial role of architectural choices in shaping the learned representation and expose the inherent biases of certain activation functions.  The success of OffsetL2, which explicitly incorporates distance calculations, serves as a powerful validation of this theoretical framework. This work, therefore, provides a foundation for developing more robust, interpretable, and statistically grounded neural network architectures, opening new avenues for both theoretical and practical advancements in the field.

