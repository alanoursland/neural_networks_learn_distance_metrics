\section{Conclusion}

This work advances our understanding of representational learning in neural networks by addressing the tension between distance-based and intensity-based representations. Through systematic experimentation, theoretical analysis, and the development of a novel architecture, we present three key contributions.

First, we demonstrate that the tension between distance-based and intensity-based representations is fundamentally shaped by geometric constraints rather than architectural choices alone. Performance differences across six architectural variants arise not from inherent biases but from how effectively architectures navigate these constraints in the feature space. This insight redefines the role of architecture in determining representational preferences.

Second, we provide theoretical and empirical evidence for why certain architectures succeed or fail under specific constraints. The catastrophic failure of ReLU2 ($47.20\%$ accuracy) reveals how intensity-based constraints can create untenable optimization landscapes, especially when inputs must align with decision boundaries under high-dimensional imbalance. Conversely, the performance gap between Abs2 ($95.35\%$ accuracy) and Abs2-Neg ($90.08\%$ accuracy) highlights the critical importance of combinatorial flexibility in leveraging multiple optimal separation points across high-dimensional spaces. These examples illustrate how geometric interactions, not intrinsic properties of activation functions, drive network performance.

Third, we introduce OffsetL2, a novel architecture explicitly incorporating Mahalanobis distance calculations. OffsetL2 directly models the geometric principles uncovered in our analysis, achieving state-of-the-art performance ($97.61\%$ accuracy) with remarkable stability ($\pm0.07\%$ standard deviation) across all variants. This success validates our theoretical framework linking neural network representations to statistical distance measures and demonstrates the power of explicitly incorporating geometric constraints into network design.

These findings have significant implications for neural architecture design. Rather than evaluating architectural components solely based on biological motivation or gradient flow, we propose a geometric perspective: analyzing the interaction between activation functions, architectural components, and the constraints of the feature space. This approach provides a principled foundation for designing more robust, interpretable, and efficient networks. 

Looking ahead, future work should explore the application of these geometric principles to deeper architectures and more complex tasks, such as natural language processing, reinforcement learning, and unsupervised representation learning. Additionally, integrating OffsetL2-like designs into large-scale systems may yield new insights into the scalability and generalizability of these ideas.

In conclusion, this research underscores the value of viewing neural networks through the lens of geometric and statistical principles. By explicitly modeling these constraints, as demonstrated with OffsetL2, we can improve performance, stability, and interpretability, paving the way for the next generation of principled machine learning architectures.
