\section{Related Work}
\label{sec:related_work}

Interpreting neural network representations through distance metrics offers a promising avenue for enhancing their interpretability and robustness. Our recent theoretical work has established a fundamental connection between neural network computations and statistical distance measures like the Mahalanobis distance \cite{mahalanobis1936generalized}. We demonstrated how linear layers with Absolute Value (Abs) activations can approximate the Mahalanobis distance, suggesting that neural networks may learn to represent data in terms of distances to learned prototypes \cite{oursland2024interpreting}. We further supported this theoretical framework with empirical evidence showing that neural networks with ReLU and Abs activations are more sensitive to perturbations that modify distance relationships in the feature space than to those that merely alter activation magnitudes \cite{oursland2024neural}. 

Alternative approaches, including Radial Basis Function (RBF) networks \cite{broomhead1988rbf}, Siamese networks \cite{bromley1994signature}, and Learning Vector Quantization (LVQ) \cite{kohonen1995learning}, have explored the explicit integration of distance metrics into neural architectures, primarily for clustering and metric learning tasks. Prototype-based models, which learn representations of classes and classify inputs based on their proximity to these prototypes, also implicitly rely on distance computations \cite{snell2017prototypical}. While these methods demonstrate the potential of distance-based learning, they have not seen widespread adoption in general-purpose deep learning architectures.

Complementing these distance-centric views, geometric interpretations of neural computation offer valuable perspectives for understanding internal representations \cite{montavon2018methods, samek2019explainable}. These approaches often analyze hyperplanes and decision boundaries to explain how networks partition and represent data. However, while providing valuable insights into network behavior, these studies often focus on networks trained under standard intensity-based assumptions. This work aims to bridge the gap between distance-based and geometric interpretations by investigating how architectural choices, such as activation functions and the explicit incorporation of distance metrics, influence the emergence of distance-based representations and shape the geometry of the learned feature space.

