\section{Related Work}

\subsection{Distance-Based Neural Network Interpretations}
The understanding of neural network representations through the lens of distance metrics has become increasingly important in machine learning, particularly for enhancing interpretability and robustness. Theoretical studies have established connections between neural network computations and statistical distance measures like the Mahalanobis distance. This research highlights how linear layers and activation functions can effectively model statistical relationships within feature spaces, offering a more statistically grounded perspective on network representations.

Alternative approaches, including Radial Basis Function (RBF) networks \cite{broomhead1988rbf} and Siamese networks \cite{bromley1994signature}, have explored the explicit integration of distance metrics into neural architectures, primarily for clustering and metric learning tasks. However, these methods often focus on specific applications, with limited generalization to broader architectures. Recent work by Oursland et al. \cite{oursland2024interpreting} proposed a theoretical framework linking general neural networks with Mahalanobis distance calculations, but empirical validation of this connection remains an active area of investigation.

Complementing these distance-centric views, geometric interpretations of neural computation offer valuable perspectives for understanding internal representations. These approaches often analyze hyperplanes and decision boundaries to explain how networks partition and represent data. However, they may not fully address the complex interplay between architectural constraints and representational biases. Our research aims to bridge this gap by systematically investigating how architectural choices influence the emergence of distance-based representations.

\subsection{Activation Functions and Learned Representations}
The selection of activation functions plays a crucial role in shaping the representations learned by neural networks. ReLU \cite{nair2010relu}, due to its simplicity and computational efficiency, has become a predominant choice in many architectures. However, its tendency to produce "dead neurons" can potentially hinder representational learning. Absolute Value (Abs) activations have been proposed as an alternative, facilitating symmetric, distance-like representations \cite{oursland2024interpreting}.

Recent studies have explored the representational differences between ReLU and Abs activations, demonstrating that networks with these activations exhibit robustness to intensity perturbations but high sensitivity to distance-based perturbations \cite{oursland2024neural}. These findings suggest an inherent inclination toward distance-based representations under certain architectural constraints.

Further research has investigated how specific architectural modifications, such as the inclusion of bias terms and Neg layers, can influence representational preferences in neural networks. While this prior work has provided valuable insights into the dynamics of representation learning, a comprehensive understanding of how these factors interact with geometric constraints in feature spaces remains an open question. This paper addresses this gap by systematically evaluating the interplay between activation functions, representational biases, and architectural constraints, providing a unified framework for understanding representation learning in neural networks.
