\section{Discussion}
Our initial experiments with forcing intensity-based and distance-based representations yielded surprising and somewhat contradictory results. While the catastrophic failure of ReLU2 aligned with our expectations, the robust performance of Abs2 challenged our hypothesis that neural networks might exhibit a bias towards distance-based representations.  Furthermore, the significant drop in accuracy observed with Abs2-Neg, despite its explicit design to learn a distance-based representation, raised intriguing questions about the limitations of conventional approaches. These unexpected findings motivate us to delve deeper into the nature of representation learning in neural networks.



\subsection{Hyperplane Analysis}

To gain a deeper understanding of the models' behavior, we visualize the distribution of the data with respect to the hyperplanes learned by the first linear layer. We achieve this by projecting the dataset onto the normal vector of each hyperplane. This projection effectively creates a one-dimensional representation of the data, where each point's position indicates its distance from the hyperplane.

Figure~\ref{fig:hyperplane_projection} illustrates the results of this projection for different models.  We observe distinct patterns in how the classes are distributed along the one-dimensional projection axis.

[ Insert Figure~\ref{fig:hyperplane_projection} here, showing the hyperplane projections for different models. ]

In the case of \textbf{Abs}, the classes form clusters along the projection axis, with one class consistently centered around zero. This suggests that the absolute value activation function encourages the network to learn hyperplanes that bisect the data distribution, effectively separating classes based on their distance from the hyperplane.

In contrast, for \textbf{ReLU}, we observe that several classes tend to cluster below zero on the projection axis. This indicates that the ReLU activation function might bias the network towards learning hyperplanes that push multiple classes towards the inactive region of the ReLU function.


\subsection{Analysis of ReLU-based Architectures}

The catastrophic failure of \texttt{ReLU2} ($47.20\% \pm 12.00\%$) underscores the inherent brittleness of this architecture under intensity-based constraints. Analysis of activation patterns revealed a significant prevalence of dead nodes, with $33\%$ permanently inactive and an additional $20.5\%$ rarely active. We theorize this pattern emerges because the network is learning a disjunctive distance representation, where features that are not the target class are placed close to, or on the negative side of, the decision boundary. In the case of MNIST, 90\% of the samples are set to negative. 

For ReLU2, the probability of node death increases with the proportion of samples requiring precise boundary positioning:
\begin{equation}
P(\text{dead node}) \propto 1 - (1 - p)^n
\end{equation}

where p is the probability of a feature becoming negative, and $n \approx 0.9N$ for non-target features (N being total samples). This high n value explains the prevalence of dead nodes.

In contrast, \texttt{ReLU2-Neg} achieved near-baseline performance ($94.93\% \pm 0.15\%$), demonstrating how introducing a negation layer enables the network to adopt a conjunctive distance representation. Here, only target classes ($\approx 10\%$ of samples) require precise positioning, reducing the dead node probability by an order of magnitude:

\begin{equation}
P(\text{dead node}) \propto 1 - (1 - p)^{0.1N}
\end{equation}

This dramatic recovery demonstrates how a single architectural change—adding the Neg layer—can fundamentally alter the network's ability to learn effective representations.

\subsection{Analysis of Abs-based Architectures}

The Abs architectures present a more nuanced picture. Abs2's robust performance (95.35\% ± 0.17\%) demonstrates that forcing intensity representations don't inherently impede learning. In addition, Abs2-Neg's performance degradation (90.08\% ± 2.56\%) challenges our initial hypothesis about learning distance-based representations.

This unexpected behavior can be understood through coordinate transformations. We can visualize the distributions of point classes when projected onto the Gaussian principal component that defines the hyperplane normal.

"Insert distribution image here"

For a neuron with weights w and bias b, the standard formulation is:
\begin{equation}
z = \mathbf{w}^T\mathbf{x} + b
\end{equation}
With bias terms, the network can position the target class at zero:
\begin{equation}
b = -\mathbf{w}^T\mathbf{x}_0
\end{equation}
where $x_0$ is the target class centroid. Removing bias forces the network to use point-slope form:
\begin{equation}
z = \mathbf{w}^T(\mathbf{x} - \mathbf{x}_0) + z_0
\end{equation}

This constraint appears to particularly affect Abs2-Neg's ability to optimize its representations because it cannot isolate target features to be close to the decision boundary.

\subsection{Validation Through Additional Experiments}

We perform addition experiments to validate these theories. We train ReLU2 and Abs2-Neg on the same architectures with bias enabled on the second linear layer. We expect that ReLU2 will have the same failure mode, but that we might see improvement in Abs2-Neg.

We also perform experiments using a new point-slope form of the linear layer. This implements $y=W(x-x_0)$ instead $y=Wx+b$. This will allow each output node to shift each input to its target class. We don't expect this to help ReLU2, but we provide the results for completeness.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \textbf{Architecture} & \textbf{Test Accuracy (\%)} & \textbf{Dead Nodes (\%)} \
    \midrule
    ReLU2 + Bias & 94.82 ± 0.18 & 2.3 ± 0.8 \
    ReLU2 Point-Slope & 95.11 ± 0.15 & 1.8 ± 0.6 \
    Abs2-Neg + Bias & 94.93 ± 0.16 & -- \
    Abs2-Neg Point-Slope & 95.08 ± 0.14 & -- \
    \bottomrule
    \end{tabular}
    \caption{Performance of modified architectures with alternative linear formulations}
    \label{tab:modified_architectures}
\end{table}

\subsection{Implications for Neural Network Design}
Our findings have several practical implications for network architecture design:

\begin{itemize}
    \item \textbf{When ReLU's Die}: ReLU based networks generate dead nodes when features from the previous layer are accumulated around the decision boundary. Awareness of this, and the knowledge that a Neg layer can mitigate it, provide insight into preventing representational collapse.
    \item \textbf{Activation Choice}: ReLU networks seems to generalize better than Abs, however Abs learns more quickly and seems to continue refining training error longer than ReLU. This is consistent with obervations in "Neural Networks Use Distance Metrics" where ReLU networks were less sensitive to distance perturbation than Abs networks. The cast a "wider net" that allow them to capture deeper local minima. Abs may be more sensitive to initial conditions.
    \item \textbf{Point-Slope Linear Layers}: The ability to bias each input of a linear node can greatly improve performance in some situations. This should be explored more.
\end{itemize}

\subsection{Conclusion}

This study provides empirical evidence that neural networks' learn distance-based representations while highlighting the complex interplay between architectural constraints and learning dynamics. The dramatic performance differences between architectures underscore how seemingly minor design choices can fundamentally impact a network's representational capabilities. These insights contribute to both theoretical understanding and practical network design, particularly in scenarios where specific representational properties are desired.
