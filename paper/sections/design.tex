\section{Experimental Design}
\label{sec:exp_design}

\subsection{The Core Tension: Distance vs.\ Intensity}
\label{sec:core_tension}
A key theoretical premise, motivated by prior work, is that hidden-layer activations in neural networks can behave like a distance metric: smaller activation values indicate a closer match to a learned prototype or decision boundary. Neural networks trained with \texttt{CrossEntropyLoss} (CEL) and \texttt{LogSoftmax} ultimately rely on an \emph{intensity-based} interpretation of logits: \emph{larger values} (whether large positive or least negative) are taken to indicate stronger feature presence.  This creates a conceptual tension:
\begin{itemize}
    \item \textbf{Distance-Based Feature Interpretation:} Smaller hidden activations $\implies$ stronger match.
    \item \textbf{Intensity-Based Feature Interpretation:} Larger hidden/logit values $\implies$ stronger presence of that feature/class.
\end{itemize}

Although the final classification in a CEL + LogSoftmax framework is always intensity-based (highest logit wins), we hypothesize that the network's \emph{internal} representations may still prefer distance-like patterns. The experiments below are designed to determine whether networks naturally favor distance or intensity encodings under various architectural constraints.

\subsection{Six Architectures to Probe Representations}
\label{sec:architectures}
To test whether networks exhibit an inherent bias toward distance-based or intensity-based representations, we design six two-layer architectures. Each uses two linear layers and either \texttt{ReLU} or \texttt{Abs} as nonlinearities in the hidden layer(s). The key difference lies in how the second (possibly final) nonlinearity and any negation (\texttt{Neg}) step are arranged:
\begin{enumerate}
    \item \(\mathbf{x} \rightarrow \text{Linear} \rightarrow \text{ReLU} \rightarrow \text{Linear} \rightarrow \mathbf{y}\)
    \item \(\mathbf{x} \rightarrow \text{Linear} \rightarrow \text{Abs} \rightarrow \text{Linear} \rightarrow \mathbf{y}\)
    \item \(\mathbf{x} \rightarrow \text{Linear} \rightarrow \text{ReLU} \rightarrow \text{Linear} \rightarrow \text{ReLU} \rightarrow \mathbf{y}\)
    \item \(\mathbf{x} \rightarrow \text{Linear} \rightarrow \text{ReLU} \rightarrow \text{Linear} \rightarrow \text{ReLU} \rightarrow \text{Neg} \rightarrow \mathbf{y}\)
    \item \(\mathbf{x} \rightarrow \text{Linear} \rightarrow \text{Abs} \rightarrow \text{Linear} \rightarrow \text{Abs} \rightarrow \mathbf{y}\)
    \item \(\mathbf{x} \rightarrow \text{Linear} \rightarrow \text{Abs} \rightarrow \text{Linear} \rightarrow \text{Abs} \rightarrow \text{Neg} \rightarrow \mathbf{y}\)
\end{enumerate}

\subsubsection{Non-Neg vs.\ Neg Variants}
\label{sec:nonneg_neg}
Because \texttt{CrossEntropyLoss} (\texttt{CEL}) ultimately requires an intensity-based output (the largest logit is chosen), the presence or absence of a \texttt{Neg} step forces the second layer to adopt different internal representations. Both \texttt{ReLU} and \texttt{Abs} produce non-negative outputs, which naturally suits a distance-like interpretation (distances cannot be negative). However, 
\begin{itemize}
    \item \textbf{With Neg:} The non-negative penultimate outputs (interpretable as “distance-like”) are flipped to negative values. Being the “least negative” among classes still makes that logit the winner in \texttt{LogSoftmax}, so the second layer is effectively \emph{forced} to represent distance (small = better match).  
    \item \textbf{Without Neg:} The network must produce positively valued logits directly to succeed under \texttt{CEL}, thereby \emph{forcing} an intensity-like representation (large = stronger match) in the second layer.
\end{itemize}
This explicit manipulation of sign flow and final activations is what systematically compels each architecture toward either distance-based or intensity-based representations at the final stage.


\subsubsection{Simple Control Models vs.\ Strictly Constrained Models}
\label{sec:control_forced}
\begin{itemize}
    \item \textbf{Models 1 \& 2 (Control):} No second nonlinearity on the final output. The network can learn either distance or intensity representations by tuning weight signs or biases.
    \item \textbf{Models 3 \& 5 (Forced Intensity):} A second \texttt{ReLU} or \texttt{Abs} ensures final outputs are non-negative. This naturally favors an intensity-like encoding (\emph{larger} activations indicate stronger presence).
    \item \textbf{Models 4 \& 6 (Forced Distance):} After a non-negative penultimate layer, a final \texttt{Neg} flips sign. This explicitly encourages a distance-like internal representation (small = strong match), which is then converted into a negative logit that can still ``win'' if it is the least negative.
\end{itemize}

\subsection{Role of Bias Removal in the Final Layer}
\label{sec:bias_removal}
A key design choice is \emph{removing the bias} in the second linear layer. This responds directly to our core question: \emph{Does the network truly adopt a distance-based representation, or is it only artificially shifting outputs to appear intensity-like?} With a bias term, the network could trivially ``translate'' any distance-like encoding (small values) into a large positive range. By disallowing bias:
\begin{itemize}
    \item We prevent the model from simply shifting negative or small activations to large positive logits.
    \item If the network wants positively valued logits, it must \textit{intrinsically} learn an intensity-based pattern or rely on a \texttt{Neg} step to flip distance-coded signals.
\end{itemize}
Hence, bias removal makes it clearer whether the final logits arise from a fundamentally distance- or intensity-like internal representation. This is crucial for teasing out the network’s \textbf{true} preference.

\subsection{Training Setup: Minimizing Confounds}
\label{sec:training_setup}
All six architectures are trained under the same minimal conditions:
\begin{itemize}
    \item \textbf{Dataset:} MNIST
    \item \textbf{Optimizer:} Stochastic Gradient Descent (full-batch updates)
    \item \textbf{Epochs:} 5000
    \item \textbf{Fixed Learning Rate} 0.001
    \item \textbf{Repetitions:} 20 runs per architecture
\end{itemize}
Using full-batch (the entire MNIST training set per gradient update) avoids the stochasticity of mini-batch sampling and reduces hyperparameter tuning. No momentum or adaptive methods (e.g., Adam) are used, and there is no early stopping. We adopt this stripped-down setting so that any differences in final performance or internal activations can be attributed to the \emph{architectural} constraints and the natural inclination (distance vs.\ intensity) of the network.

\subsection{Expected Outcome \& Significance}
\label{sec:expected_outcomes}
Based on prior theory, we hypothesize that neural networks may have an inherent inclination toward distance-like representations (\( \text{small} = \text{strong match} \)), yet the final classification objective (\texttt{CrossEntropyLoss} with \texttt{LogSoftmax}) enforces an intensity-based view (largest logit = chosen class). Our experiment manipulates the architecture to \emph{force} either a distance-based or intensity-based encoding in the second layer (via \texttt{Neg} or non-negative activations). 

We will compare the \emph{final classification accuracy} of each architecture under identical training conditions. If the models forced into a distance representation can match or exceed the performance of models forced to use intensity, this would suggest that distance-based encodings are not only feasible but may be naturally preferred. Conversely, if the forced-intensity models dominate, it would indicate that networks more readily adopt an intensity-style representation. 

Overall, this experimental design systematically manipulates activation sign constraints, bias usage, and final-layer flipping to \emph{differentiate} a distance-based encoding from the standard intensity-based output of cross-entropy classification. The forthcoming results and analyses section will detail how effectively networks can adopt or abandon smaller vs.\ larger activations to signify “feature presence,” thereby elucidating fundamental representational biases in neural networks.
