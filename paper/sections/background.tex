\section{Background}

\subsection{Motivation and Context}
Deep neural networks have been shown to learn complex internal representations that significantly impact their performance and interpretability. While early neural models such as the McCulloch-Pitts neuron  and the perceptron  conceptualized activations in an \emph{intensity-based} manner (larger activation $\implies$ stronger feature), recent theoretical work suggests that \emph{distance-based} interpretations may arise naturally in hidden layers. Understanding these hidden-layer biases has implications for optimization stability, generalization, interpretability, and the design of future architectures.

In the broader context of modern deep learning, representations learned by neural networks are central to their success. However, questions remain about how these representations are shaped by architectural choices (e.g., activation functions, presence/absence of bias terms) and by training objectives (e.g., \texttt{CrossEntropyLoss} forcing “largest logit” classification). Addressing these questions is crucial for:
\begin{itemize}
    \item Designing networks that leverage distance-like encodings, potentially improving robustness or interpretability.
    \item Understanding why certain networks catastrophically fail under standard intensity-based assumptions.
    \item Informing practical decisions on activation functions, normalization schemes, and bias term usage to align with the desired representational preference.
\end{itemize}

\subsection{Distance vs. Intensity Representations}

\paragraph{Distance-Based Features}
A \emph{distance-based} perspective interprets smaller activations as indicating stronger matches to learned prototypes or centroids. Mathematically, one might define the activation $a(x)$ for input $x$ as:
\begin{equation}
    a(x) = \|W(x - \mu)\|_p,
    \label{eq:distance-based}
\end{equation}
where $\mu$ is a learned prototype (or mean) and $W$ a scaling matrix. Activations closer to zero imply that $x$ is more similar to the prototype under the chosen norm. This approach aligns with statistical metrics such as Mahalanobis distance and is theoretically grounded in multivariate analysis.

\paragraph{Intensity-Based Features}
Historically, neural networks have treated larger neuron activations as indicative of stronger feature detections. For a linear layer followed by an activation function $f(\cdot)$, the activation is:
\begin{equation}
    a(x) = f(Wx + b),
    \label{eq:intensity-based}
\end{equation}
where $Wx + b$ is the affine transformation of $x$. Under typical classification objectives like \texttt{CrossEntropyLoss} with \texttt{LogSoftmax}, the largest logit determines the predicted class, thereby \emph{reinforcing} intensity-based outputs at the final layer.

% \begin{figure}[ht]
%     \centering
%     % Placeholder for an illustrative figure
%     \includegraphics[width=0.75\textwidth]{placeholder_distance_representation.png}
%     \caption{(a) Distance-based representation: smaller activations correspond to closer proximity to learned prototypes. 
%     (b) Intensity-based representation: larger activations indicate stronger feature presence.}
%     \label{fig:distance_representation}
% \end{figure}

\subsection{Mathematical and Statistical Foundations}

\paragraph{Mahalanobis Distance}
Distance-based representations can often be linked to the Mahalanobis distance, a generalization of Euclidean distance that accounts for covariance among features:
\begin{equation}
    D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)},
    \label{eq:mahalanobis-1}
\end{equation}
where $\mu$ is the mean (or prototype) and $\Sigma$ is the covariance matrix. 
Decomposing $\Sigma$ via eigendecomposition ($\Sigma = V \Lambda V^T$) yields:
\begin{equation}
    D_M(x) = \|\Lambda^{-\frac{1}{2}} V^T (x - \mu)\|_2.
    \label{eq:mahalanobis-2}
\end{equation}
In principle, linear transformations in a neural layer could approximate parts of this distance computation. (For brevity, we omit the detailed derivation of the decomposition, and refer the reader to  for more details.)

\paragraph{Geometric Interpretations and Bias Terms}
Neural networks can be viewed as mapping inputs into intermediate latent spaces where geometry (e.g., hyperplanes, boundaries) emerges. Bias terms $b$ in the affine transformation $Wx + b$ shift these hyperplanes:
\begin{equation}
    y = Wx + b.
\end{equation}
Such shifts can transform an otherwise distance-like internal representation (where smaller values indicate closeness) into intensity-like logits (where larger values indicate stronger predictions). This perspective underscores the importance of bias terms in shaping representational preference.

\paragraph{Whitening and Normalization}
Normalization techniques (e.g., batch normalization, whitening) can reshape the data manifold into forms more amenable to distance-like calculations. These transformations adjust the distribution of features, which can facilitate or suppress distance-based interpretations in hidden layers.

\subsection{Network Components and Representational Impact}

\paragraph{Activation Functions}
Choice of activation function imposes distinct geometric and numerical properties:
\begin{itemize}
    \item \textbf{ReLU}~: $f(x) = \max(0, x)$. This half-space activation naturally aligns with intensity-based ideas. However, ReLU networks can suffer from ``dead neurons,'' where weights are never updated if the input consistently lies in the negative regime.
    \item \textbf{Abs}: $f(x) = |x|$. This symmetric activation can preserve information about magnitude independent of sign, aligning more closely with distance-based interpretations and preventing neurons from dying.
\end{itemize}

\paragraph{Neg Layers}
A so-called ``Neg layer'' can flip non-negative activations to negative logits, emphasizing that lower values represent stronger matches (distance-like). This is sometimes introduced to force the network to interpret closeness in hidden units as ``strong activation'' in a reversed sense.

\paragraph{Classification Framework}
While hidden layers might favor distance-like encodings, the final classification layer typically uses \texttt{CrossEntropyLoss} combined with \texttt{LogSoftmax}, enforcing an \emph{intensity-based} paradigm for the ultimate decision. The largest output logit determines the predicted class, which can conflict with any distance-based preference in earlier layers.

\subsection{Experimental Architecture and Hypotheses}

Here we integrate both the detailed methodology of Draft 2 and the visual/structural clarity of Draft 3. Our experimental design systematically varies:
\begin{enumerate}
    \item \textbf{Activation function (ReLU vs. Abs)}: to test whether the network naturally learns distance-like or intensity-like encodings depending on nonlinearity.
    \item \textbf{Neg layers}: to explicitly force distance-based interpretation by flipping signs.
    \item \textbf{Bias terms}: to investigate how bias removal alters the representational preference, preventing a shift from distance-like internal codes to intensity-like outputs.
\end{enumerate}

% \begin{figure}[ht]
%     \centering
%     % Placeholder for hyperplane illustration
%     \includegraphics[width=0.50\textwidth]{placeholder_hyperplane_bias.png}
%     \caption{Illustration of how bias terms can shift hyperplanes, transforming distance-like intermediate representations into intensity-based logits.}
%     \label{fig:hyperplane_bias}
% \end{figure}

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{Activation} & \textbf{Neg Layer} & \textbf{Bias Terms} \\
\hline
Model 1 & ReLU & No & Yes \\
Model 2 & Abs & No & Yes \\
Model 3 & ReLU & Yes & Yes \\
Model 4 & Abs & Yes & Yes \\
Model 5 & ReLU & No & No \\
Model 6 & Abs & No & No \\
\hline
\end{tabular}
\caption{Overview of experimental configurations systematically varying activation functions, Neg layers, and bias terms.}
\label{tab:architectures}
\end{table}

\subsubsection{Hypotheses}
Building on prior work, we propose:
\begin{enumerate}
    \item \textbf{Hidden layers favor distance-based representations}: Even under conventional intensity-driven loss functions, intermediate layers may minimize distance to prototypes.
    \item \textbf{Enforcing intensity-based outputs may lead to optimization challenges}: Removing or flipping bias terms can expose potential conflicts between distance vs. intensity objectives.
    \item \textbf{Explicit distance-based architectures can match or exceed traditional performance}: Architectures designed for distance-based encodings may yield more stable or interpretable solutions.
\end{enumerate}

\subsection{Related Work and Literature Review}
Several lines of research inform our study of representational biases:
\begin{itemize}
    \item \textbf{Representational Learning and Embeddings:} Previous works on embeddings and manifold learning have highlighted the geometric aspects of deep representations. 
    \item \textbf{Activation Function Studies:} Exploring how ReLU, LeakyReLU, Abs, or other nonlinearities impact learned representations has been a focal point for improved stability and performance.
    \item \textbf{Bias Terms and Geometry:} Biases have been studied for their role in shifting decision boundaries and altering interpretability, as seen in prior work on margin theory and linear separators.
    \item \textbf{Distance Measures in Neural Networks:} Recent papers have investigated how network layers might implicitly compute distances, especially in tasks involving clustering or metric learning.
\end{itemize}
We position our work at the intersection of these research directions, examining whether standard architectures \emph{naturally} adopt distance-like encodings, and how minor architectural changes (activation, bias usage, Neg layers) can either enhance or suppress these tendencies.

\subsection{Summary}
In summary, we build upon a strong theoretical framework (drawn from Draft 2) while integrating visual aids (from Draft 3) and expanded motivation. Our aim is to:
\begin{itemize}
    \item Examine the interplay between distance and intensity representations in hidden layers.
    \item Investigate how bias terms and Neg layers modulate these tendencies.
    \item Provide insights that help researchers design models aligning with the desired representational bias.
\end{itemize}
The following sections detail our experimental protocol, results, and analyses, which collectively address whether and why neural networks prefer distance-based or intensity-based internal encodings under standard training objectives.

