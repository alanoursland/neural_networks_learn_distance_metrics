\section{Background}
\label{sec:background}

Understanding the foundations of how neural networks learn and represent data is critical to improving their interpretability and robustness. In this section, we introduce the theoretical groundwork that frames our exploration of distance-based and intensity-based representations, providing the necessary context for the experiments and analyses to follow.

\subsection{Features, Representations, and Distance Metrics}

In this work, we draw a distinction between features and their representation within a neural network. We define features as inherent characteristics of the data that can be captured by statistical distance metrics, reflecting underlying relationships and structures within the data. Representations, on the other hand, refer to how the network encodes these features, which can be either intensity-based or distance-based. Intensity-based representations encode features through the magnitude of activations, while distance-based representations encode features through proximity to learned prototypes, as measured by a distance metric.

We argue that neural networks fundamentally learn distance features, features grounded in statistical distance metrics that quantify the similarity or dissimilarity between data points and learned prototypes. From this perspective, there is no inherent notion of a statistical "intensity feature." What are commonly interpreted as intensity representations are, in fact, manifestations of underlying distance relationships.

What are typically perceived as intensity-based representations can be reinterpreted as disjunctive sets of distance features. In logical terms, this corresponds to representing a concept in Disjunctive Normal Form (DNF), a concept formalized by early work in Boolean algebra \cite{post1921introduction}. For instance, if a network appears to learn an intensity representation for class $a$, it can be interpreted as implicitly learning the negation of distances to other classes: $\lnot(b or c) = \lnot b and \lnot c$, where $b$ and $c$ represent distance features associated with other classes. We posit that $a$ cannot be directly represented by a linear intensity metric, and that the perceived intensity representation is an artifact of underlying distance relationships.

In contrast, a distance-based representation, as advocated in this work, can be viewed as a conjunctive set of distance features. This aligns with representing a concept in Conjunctive Normal Form (CNF). In this approach, the network directly encodes proximity to a specific prototype or set of prototypes. For example, a distance representation for classes $a$ would simply entail a small distance to the prototype(s) associated with $a$. This is represented logically as simply $a$.

\subsection{Distance vs. Intensity Representations}
\label{subsec:dist-intensity-rep}

While a precise mathematical definition of "intensity-based representation" remains elusive, the concept is implicitly embedded in many common practices within the field. Operationally, we can define intensity-based interpretations as those that treat larger neural activations as indicative of a stronger presence, higher probability, or greater certainty of a particular feature or class. This view is prevalent in the interpretation of output layers in classification tasks, where the class corresponding to the highest activation (often after a softmax) is taken as the network's prediction, a method reinforced by the use of one-hot encoded labels in datasets like MNIST. This practice is further solidified by the use of loss functions like Cross-Entropy, which encourage higher activations for the correct class during training.  Furthermore, techniques like feature visualization via activation maximization \cite{erhan2009visualizing, olah2017feature} rely on the assumption that inputs generating the highest activations in specific neurons or layers are most representative of the learned features. This intensity-based view also manifests in the common practice of interpreting pixel values in image datasets directly as intensities, where brighter pixels are often assumed to correspond to more significant features. It is important to note that this intensity-based interpretation, while prevalent, is ultimately an assumption about the meaning of neural activations. It is not inherently dictated by the underlying mathematical operations of the network but rather imposed upon them.

In contrast, a distance-based representation interprets smaller activations as indicating greater similarity to learned prototypes or closer proximity to decision boundaries. This perspective aligns with statistical distance metrics, such as the Mahalanobis distance \cite{mahalanobis1936generalized}, where $f(x)$ is inversely proportional to the distance of the input $x$ from one or more learned prototypes $\mu$. These prototypes effectively define the decision boundaries within the feature space. Mathematically, this can be expressed as:
\begin{equation}
f(x) = |W(x - \mu)|_p,
\end{equation}
where $W$ is a learned scaling matrix that shapes the distance metric according to the underlying data structure, $\mu$ is the prototype, and $p$ denotes the norm. This framing reinterprets activations as proximity to learned features, emphasizing geometric relationships within the latent space.

While the absolute value function (Abs) provides a direct mathematical representation of distance, the widely used Rectified Linear Unit (ReLU) can also be interpreted within this framework. As shown in the relationship $Abs(x) = ReLU(x) + ReLU(-x)$, the Abs function can be decomposed into two ReLU functions, one operating on the positive part and the other on the negative part of the input. This suggests that even when using ReLU, the network can still implicitly encode information about the distance.

\subsection{Theoretical Foundations: Mahalanobis Distance}

The Mahalanobis distance offers a statistically grounded framework for understanding distance-based representations. Defined as:
\begin{equation}
    D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)},
\end{equation}
where $\Sigma$ is the covariance matrix, the Mahalanobis distance adjusts for feature correlations and scales. Neural network layers can approximate components of this metric through linear transformations and normalization techniques \cite{oursland2024interpreting}.

Decomposing $\Sigma$ via eigendecomposition $\Sigma = V\Lambda V^T$, where $V$ contains eigenvectors and $\Lambda$ is a diagonal matrix of eigenvalues, allows the distance to be expressed as:
\begin{equation}
    D_M(x) = \| \Lambda^{-1/2} V^T (x - \mu) \|_2.
\end{equation}
This connection reveals how linear layers with activation functions such as Absolute Value (Abs) can model statistical distances, linking neural networks to interpretable, distance-based computations.

\subsection{Geometric Interpretations of Representations}

Neural networks map inputs into latent spaces where learned features are geometrically structured by hyperplanes. In a distance-based interpretation, these hyperplanes are not merely separators of classes, but rather work together to define manifolds where distances to learned prototypes determine the representation. 

The equation y = Wx + b, where x is N-dimensional and y is a scalar, defines an N-dimensional hyperplane embedded in an (N+1)-dimensional space. The decision boundary of the hyperplane is defined as the region where it intersects the data plane where the activation function equals zero (e.g., $0 = Wx + b$ in a linear layer). This decision boundary is an (N-1)-dimensional subspace of the input space. Each hyperplane can be uniquely described by a set of `N` linearly independent points that it intersects. $N-1$ of these points can be on the decision boundary with the final point resting outside the data plane at ${x=0, y=b}$. Points on this decision boundary, can be viewed as feature protypes that encode meaningful relationships between input features and their representation in the latent space. These prototypes may not be similar in the way humans typically perceive similarity, as they are defined by their shared location within a specific distance metric space rather than by superficial resemblance. Each prototype can be seen as a representative example within a particular region of the data plane, capturing the essential features that define that region.

In the absence of a bias term (such that $b=0$), each hyperplane is constrained to pass through the origin of the latent space. This effectively makes the origin a fixed prototype for each hyperplane. While this constraint might appear to limit flexibility, in high-dimensional spaces the impact on the overall representational power is minimal, considering that each hyperplane still intersects `N-2` other linearly independent points on its decision boundary, in addition to the origin and a point off the data plane ${x!=0, y!=0}$. Although these prototype points are theoretically important for understanding the learned representations, recovering them from hyperplanes in high-dimensional spaces is often computationally infeasible due to to the exponential growth of the search space with increasing dimensionality.

\subsection{Activation Functions and Representational Preferences}

Activation functions play a critical role in shaping the representational biases of neural networks:
\begin{itemize}
    \item \textbf{ReLU}: $f(x) = \max(0, x)$. ReLU is often associated with intensity-based interpretations, emphasizing large, positive activations. However, as discussed in \ref{subsec:dist-intensity-rep}, ReLU can also be viewed through a distance-based lens. The zeroing out of negative activations effectively creates a region of the feature space where the activation is zero, corresponding to inputs that lie on one side of the decision boundary. The magnitude of the positive activations can be interpreted as encoding proximity to learned prototypes within the region where the activation is non-zero. Despite this potential for a distance-based interpretation, ReLU suffers from "dead neurons," where nodes become inactive if inputs consistently lie in the negative regime, potentially hindering the learning process.
    \item \textbf{Absolute Value (Abs)}: $f(x) = |x|$. Abs offers a more direct connection to distance-based representations by symmetrically reflecting negative activations, preserving magnitude information. Unlike ReLU, Abs maintains information about the magnitude of the distance from the decision boundary, regardless of direction. This ensures all neurons remain active and contribute to the representation, providing a more complete picture of the input's position relative to learned prototypes.
    \item \textbf{Neg Layers}: $f(x) = -x$. Neg layers invert activations. They reverse the order of activations, transforming larger values into smaller (more negative) values and vice-versa. They convert positive distance representations into negative intensity representations.
\end{itemize}

\subsection{Connecting Theory to Experiments}

This theoretical foundation frames our experimental analysis, where we systematically investigate how architectural choices, such as activation functions and bias terms, affect representational biases. By probing these factors, we aim to elucidate the fundamental principles driving neural network learning and provide a unified framework for understanding distance-based and intensity-based representations.
