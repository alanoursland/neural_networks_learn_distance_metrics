This paper challenges the conventional intensity-based interpretation of neural network activations, where larger values signify stronger feature presence. We posit that networks are fundamentally biased towards distance-based representations, where smaller activations indicate closer proximity to learned prototypes. To test this hypothesis, we conducted experiments with six MNIST architectural variants constrained to learn either distance or intensity representations. Our results reveal that the underlying representation affects model performance. We develop a novel geometric framework that explains these findings and introduce OffsetL2, a new architecture based on Mahalanobis distance equations, to further validate this framework. This work highlights the importance of considering distance-based learning in neural network design.



