Neural networks have historically relied on intensity-based representations, where larger activations indicate stronger feature presence. Recent theoretical work, however, suggests that distance-based representations—where smaller activations signify stronger feature matches—may naturally emerge in hidden layers. This paper systematically investigates this representational tension by evaluating six architectural variants, revealing that the success of these representations depends not on inherent architectural biases but on how networks navigate geometric constraints within the feature space. We provide theoretical and empirical insights into why certain architectures catastrophically fail under intensity constraints (47.20\% accuracy) while others maintain robust performance (95.35\% accuracy). To address these challenges, we introduce \textit{OffsetL2}, a novel architecture explicitly integrating Mahalanobis distance computations. OffsetL2 achieves remarkable performance and stability for such a small model, attaining 97.61\% accuracy on MNIST with minimal variance ($\pm$0.07\% standard deviation). These results validate our geometric framework and underscore the potential of explicitly modeling statistical distance measures in neural architectures to enhance performance and interpretability.
