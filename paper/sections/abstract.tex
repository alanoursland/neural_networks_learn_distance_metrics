Neural networks have historically relied on intensity-based representations, where larger activations indicate stronger feature presence. Recent theoretical work, however, suggests that distance-based representations—where smaller activations signify stronger feature matches—may naturally emerge in hidden layers. This paper systematically investigates neural network feature learning through six architectural variants, revealing that networks naturally learn statistical distance metrics, with their success determined by how effectively they can represent and leverage these distance-based features. We provide theoretical and empirical insights into why certain architectures catastrophically fail under intensity constraints (47.20\% accuracy) while others maintain robust performance (95.35\% accuracy). To address these challenges, we introduce \textit{OffsetL2}, a novel architecture explicitly integrating Mahalanobis distance computations. OffsetL2 achieves remarkable performance and stability for such a small model, attaining 97.61\% accuracy on MNIST with minimal variance ($\pm$0.07\% standard deviation). These results validate our geometric framework and underscore the potential of explicitly modeling statistical distance measures in neural architectures to enhance performance and interpretability.
