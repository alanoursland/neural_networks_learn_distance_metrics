Work on Representation Learning:

Bengio, Y., Courville, A., & Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8), 1798-1828.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).

Work on Neural Network Interpretability: 

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). " Why should i trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144).

Zeiler, M. D., & Fergus, R. (2014, September). Visualizing and understanding convolutional networks. In Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 (pp. 818-833). Springer International Publishing. 

Work on Distance Metric Learning: 

Kulis, B. (2013). Metric learning: A survey. Foundations and Trends® in Machine Learning, 5(4), 287-364.

Weinberger, K. Q., & Saul, L. K. (2009). Distance metric learning for large margin nearest neighbor classification. Journal of machine learning research, 10(2).   

Xing, E. P., Ng, A. Y., Jordan, M. I., & Russell, S. (2003). Distance metric learning, with application to clustering with side-information. Advances in neural information processing systems, 15.   

Neural Networks and Distance Metrics:

Papernot, N., & McDaniel, P. (2018, April). Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. In ICLR 2018 Workshop Track Proceedings.

Goldberger, J., Hinton, G. E., Roweis, S. T., & Salakhutdinov, R. R. (2005). Neighbourhood components analysis. Advances in neural information processing systems, 17.

Hoffer, E., & Ailon, N. (2015). Deep metric learning using triplet network. International Workshop on Similarity-Based Pattern Recognition. Springer, Cham.   

Interpretability and Robustness:

Moosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. (2017). Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1765-1773).   

Jacot, A., Gabriel, F., & Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31.   

Erhan, D., Bengio, Y., Courville, A., & Vincent, P. (2009). Visualizing higher-layer features of a deep network. University of Montreal, 1341(3), 1.1

Olah, C., Mordvintsev, A., & Schubert, L. (2017). Feature visualization. Distill, 2(11), e7.

Stochastic Gradient Descent (SGD):
Original Paper (Robbins and Monro): Robbins, H., & Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, 400-407. This is the foundational paper that introduced the concept.
Optimization Book: Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization methods for large-scale machine learning. Siam Review, 60(2), 223-311. This paper provides a more recent overview and discusses SGD in the context of large-scale machine learning.
Deep Learning Book: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press. Chapter 8 of this book has a good explanation of SGD and its variants.
CrossEntropyLoss:
PyTorch Documentation: You can cite the official PyTorch documentation for CrossEntropyLoss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html   
Deep Learning Book: Again, Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press. Chapter 6 discusses Cross-Entropy as a loss function for training neural networks.
Information Theory Book (Cover and Thomas): Cover, T. M., & Thomas, J. A. (2006). Elements of information theory. John Wiley & Sons. This book provides a more theoretical background on Cross-Entropy from an information theory perspective. You might cite this if you want to emphasize the connection to information theory in your paper.

MNIST Normalization

Here are some suggestions for citations:

LeCun's original paper: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324. You could check how normalization was handled in this foundational work.   
Goodfellow, Bengio, and Courville's Deep Learning book: Check how they describe the MNIST dataset setup.
General machine learning practice: You might also find a reference that describes common data preprocessing practices in machine learning, including normalization.

I would recommend citing the original Robbins and Monro paper for historical context, followed by either the Bottou et al. review or the Goodfellow et al. book chapter for a more modern treatment of SGD in deep learning. For CrossEntropyLoss, citing the PyTorch documentation is essential, and you can add the Goodfellow et al. book or Cover and Thomas for additional context.